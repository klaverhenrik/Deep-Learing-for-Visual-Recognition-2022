{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"},"colab":{"name":"Lab2_FeatureExtractionAndTransferLearning.ipynb","provenance":[{"file_id":"16riYHbrxfQ5shWiEBTWl5WH9aOxnghBU","timestamp":1630833979888},{"file_id":"1UHpSxmUWUuBCORqC19Kb9POvu-ZWRhYy","timestamp":1630752860143},{"file_id":"https://github.com/klaverhenrik/Deep-Learning-for-Visual-Recognition-2021/blob/main/Lab2_FeatureExtractionAndTransferLearning.ipynb","timestamp":1630748870256},{"file_id":"https://github.com/aivclab/dlcourse/blob/master/Lab2_FeatureExtractionAndTransferLearning.ipynb","timestamp":1629976702215}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jdTAiE71iHF3"},"source":["# Feature extraction and transfer learning with pre-trained CNNs\n","Before proceeding *REMEMBER TO ENABLE GPU IN THE RUNTIME ENVIRONMENT:* Go to Runtime -> \"Change runtime type\" and select GPU as hardware acelerator."]},{"cell_type":"markdown","metadata":{"id":"v7tmk836r9sK"},"source":["## Task 1: Downloading images from DuckDuckGo search engine"]},{"cell_type":"markdown","metadata":{"id":"OZn30wetMkxe"},"source":["Our starting point is to find some pictures of objects that we want our neural network to recognize. For that purpose we will be using a Python package that comes with the book [\"Deep Learning for Coders\"](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527). You'll first need to install that package using ``pip``:"]},{"cell_type":"code","metadata":{"id":"Tc7woIJ4OWNr"},"source":["!pip install fastbook --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gJhwhzY2O7GJ"},"source":["To get a list of 100 URLs of cat images using the [DuckDuckGo](https://duckduckgo.com/) search engine, run the following code:"]},{"cell_type":"code","metadata":{"id":"mtuhIYewPItF"},"source":["from fastbook import *\n","urls = search_images_ddg('cat', max_images=100)\n","print(f'Number of URLs: {len(urls)}')\n","print(f'First URL {urls[0]}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IIF09v0JPmgv"},"source":["To download the image corresponding to the first URL, run this:"]},{"cell_type":"code","metadata":{"id":"rcC5MZJFPtzK"},"source":["download_url(urls[0], 'cat.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tAqrkRVVP0dZ"},"source":["Let's display it also"]},{"cell_type":"code","metadata":{"id":"eQuJMvwoP3HS"},"source":["import cv2\n","from matplotlib import pyplot as plt\n","\n","img = cv2.imread('cat.jpg')\n","plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)); # Recall OpenCV reads images as BGR\n","plt.axis('off');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5a2wNWh4zOiz"},"source":["##Task 2: Mount your Google Drive, create data directory, and download training images\n","Save this notebook to your Google Drive by selecting \"Save\" or \"Save a copy in Drive\" in the Files menu. If you want to store data permanently, you also need to mount your Google Drive, which can be done as follows:\n","\n","**Note:** In my browser, the copy-button that displays next to the authorizatin code doesn't work properly. So I copied the code manually."]},{"cell_type":"code","metadata":{"id":"eYezthubzNRt"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LJjrzCdf0WSe"},"source":["Create a directory to store the downloaded images in. I prefer to use [pathlab](https://docs.python.org/3/library/pathlib.html), but Google Colab also allows you to run normal shell commands like ``mkdir`` if you prefix the command with ``!`` (see ``ls`` example below)."]},{"cell_type":"code","metadata":{"id":"rH00yY6d0dWi"},"source":["from pathlib import Path\n","root = '/content/gdrive/My Drive/' # Don't change this\n","#root = '/content/' # Alternative solution if mounting your Drive doesn't work\n","data_dirname = 'data' # Change as you like\n","p = Path(root + data_dirname)\n","p.mkdir(exist_ok=True) # should \"/content/gdrive/My Drive/data\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ih6YdL9UieTV"},"source":["To check that the data directory was created successfully, you could ``ls`` the parent directory in a shell and and verify that ``data``is there:"]},{"cell_type":"code","metadata":{"id":"XqV0bIIuiUPP"},"source":["!ls -l \"/content/gdrive/My Drive\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ujbRRjo8RzVO"},"source":["Now, lets define the object categories that we are interested in (feel free to modify).\n","\n","**IMPORTANT**: Class names must appear in alpha-numeric order to be compatible with the class assignment of the image generator below."]},{"cell_type":"code","metadata":{"id":"m_tbNi5DR7al"},"source":["classes = ['cat','dog','horse'] # DuckDuckGo search terms"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EXrEwFSPPb66"},"source":["Then download 100 images from each category (this will take some time). Note that images from each category will be placed in seperate subfolders of the data directory.\n","\n","**IMPORTANT:** In case the download fails and you have to clear the data directory, uncomment and run these commands (remember to prefix with !):\n","\n","```\n","!rm -rf \"/content/gdrive/My Drive/data\"\n","!mkdir \"/content/gdrive/My Drive/data\" # make new empty data directory\n","````"]},{"cell_type":"code","metadata":{"id":"lZ8co4D3Pjnw"},"source":["# Download images\n","max_images = 100\n","count = 0\n","for idx, category in enumerate(classes):\n","  print(category)\n","  folder = category\n","  dest = p/folder\n","  dest.mkdir(parents=True, exist_ok=True)\n","  urls = search_images_ddg(category, max_images=max_images)\n","  download_images(dest,urls=urls,max_pics=max_images)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HTCweK_saQB7"},"source":["Verify images (remove files that cannot be opened)"]},{"cell_type":"code","metadata":{"id":"VDfhtb2FZcx_"},"source":["import cv2\n","import os\n","for c in classes:\n","  print(c)\n","  filelist = [x for x in (p/c).iterdir() if x.is_file()]\n","  for f in filelist:\n","    img = cv2.imread(str(f))\n","    if img is None:\n","      print(f'Failed to open {f}. Deleting file')\n","      os.remove(str(f)) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e1ZGqyqR2HS2"},"source":["## Task 3: Set up neural network for feature extraction\n","We will be using a deep learning framework, called [Keras](https://keras.io/). Keras is a high-level neural network API, written in Python and capable of running on top of [TensorFlow](https://www.tensorflow.org/), CNTK, and Theano.\n","\n","A common and highly effective approach to deep learning on small image datasets is to leverage a pre-trained network. A pre-trained network is simply a saved network previously trained on a large dataset, such as the [ImageNet dataset](http://www.image-net.org/) (1.4 million labeled images and 1000 different classes). If this original dataset is large enough and general enough, then the spatial feature hierarchy learned by the pre-trained network can effectively act as a generic model of our visual world, and hence its features can prove useful for many different computer vision problems, even though these new problems might involve completely different classes from those of the original task.\n","\n","In our case, we will consider a convolutional neural network (CNN) trained on ImageNet. We will use the MobileNet architecture, but there are other models that you could use as well. Take a look here: https://keras.io/applications\n","\n","There are two ways to leverage a pre-trained network: feature extraction and fine-tuning. We will be covering both of them today. Let's start with feature extraction.\n","\n","Feature extraction consists of using the representations learned by an existing neural network to extract interesting features from new samples. These features are then run through a new classifier, which is trained from scratch. This could be any classifier, such as K-Nearest Neighbours (KNN).\n","\n","Traditional CNNs are divided into two parts: they start with a series of convolution and pooling layers, and they end with a densely-connected classifier. The first part is often referred to as the \"encoder\", \"feature extractor\" or \"convolutional base\" of the model. In the case of CNNs, \"feature extraction\" will simply consist of taking the convolutional base of a previously-trained network, running the new data through it, and training a new classifier on top of the output. The second part of the network, called the \"decoder\" or \"top layers\", is ignored for now. We will be using it for fine-tuning (Transfer Learning) in Task 10.\n","\n","First, let's download and instantiate the pre-trained MobileNet without the top layers (i.e., without the decoder):"]},{"cell_type":"markdown","metadata":{"id":"B0fPGZNkIG53"},"source":["**IMPORTANT:** For some reason, you may sometimes need to downgrade tensorflow to use keras. To do this, uncomment and run the following command:"]},{"cell_type":"code","metadata":{"id":"59RN53GrIcq8"},"source":["#!pip install --upgrade tensorflow==1.8.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YxeBWYX76iCI"},"source":["from tensorflow.keras.applications.mobilenet import MobileNet\n","\n","conv_base = MobileNet(weights='imagenet',\n","                      include_top=False,\n","                      input_shape=(120, 120, 3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pn92MTLj62fD"},"source":["Let's summarize the model used for feature extraction:"]},{"cell_type":"code","metadata":{"id":"KxrNI8bk69Vj"},"source":["conv_base.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PqOu5xum3T-5"},"source":["That's a lot to digest if you haven't seen a convolutional neural network before. By the end of the course, you will know what it all means. For now, see if you can figure out the answers to these questions:"]},{"cell_type":"markdown","metadata":{"id":"xm_z3RAZ7DBI"},"source":["### Questions 3.1\n","1. What is the expected shape of the input image?\n","2. What is the shape of the output of the model?\n","3. What happens to the output shape if you double the size of the input image?\n","4. Can you guess what the None dimension is used for?"]},{"cell_type":"markdown","metadata":{"id":"xSNV10j_8AFc"},"source":["##Task 4: Preprocessing\n","Many neural networks expect the input image to have a fixed, pre-defined shape. Also, the pixel intensities are assumed to be in a fixed range.\n","\n","For reasons that will become clear later in the course, neural networks do not work well on images in which the intensities lie in the standard range from 0 to 255. Instead, we want the intensities to be centered around zero. Typical intensity ranges are -127.5 to 127.5 or -1 to 1, but it depends on the chosen neural network architecture. Each pre-trained network in Keras comes with its own *preprocessor*, which assures that the intensities are scaled correctly for that particular network.\n","\n","In summary, *preprocessing* refers to the step of preparing the image to be fed into the network by making sure the image has the right shape, and that the intensities are in the correct range.\n","\n","Let's load an image, preprocess it, and feed it through the network:"]},{"cell_type":"code","metadata":{"id":"A79c9Ozj8yCV"},"source":["import numpy as np\n","from keras.preprocessing import image\n","from keras.applications.mobilenet import preprocess_input\n","\n","# Pick first image of first class (i.e., cat)\n","filelist = [x for x in (p/classes[0]).iterdir() if x.is_file()]\n","img_path = filelist[1]\n","print(f\"File path: {img_path}\")\n","\n","# Load image and preprocess it\n","img = image.load_img(img_path, target_size=(120, 120))\n","img_data = image.img_to_array(img)\n","img_data = np.expand_dims(img_data, axis=0)\n","img_preprocessed = preprocess_input(img_data.copy())\n","\n","# Feed preprocessed image through CNN encoder to get a new feature representation\n","mobilenet_features = conv_base.predict(img_preprocessed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SZR30bOf-MIS"},"source":["### Questions 4.1\n","1. What is the range of the pixel values before and after preprocessing?\n","2. So what formula do you think is used to pre-process the pixel values?\n","3. What is the order of the color channels? (you could compare with ``cv2.imread``, which we know reads images as BGR)\n","4. What is the size of the input image (``img_data`` after calling ``np.expand_dims``)?\n","5. What is the size of the calculated feature representation (`mobilenet_features`)?\n","6. So what is the reduction in dimensionality after feature extraction?"]},{"cell_type":"markdown","metadata":{"id":"Uk05mlKTr2U5"},"source":["##Task 5: Feature maps\n","As we have seen above, a convolutional neural network consists of many layers. Each layer performs some mathematical operation on the output of the previous layer. The operations have names like ``Conv2D`` , ``BatchNormalization``, ``ReLU``, and ``DepthwiseConv2D``, which you will learn about during the course.\n","\n","The output of a layer is referred to as a **feature map**.\n","\n","Let's look at some feature maps produced by the first layer:"]},{"cell_type":"code","metadata":{"id":"DeGt4WcVvjz_"},"source":["from keras import Model\n","\n","def show_feature_maps_from_layer(layer_name='conv1',img=img_preprocessed):\n","  dummy_model = Model(inputs=conv_base.input, outputs=conv_base.get_layer(layer_name).output) \n","  out = (dummy_model.predict(img)).squeeze()\n","  height = out.shape[0]\n","  width = out.shape[1]\n","  num_channels = out.shape[2]\n","  print(f'Feature map size: {height}x{width}x{num_channels}')\n","\n","  plt.figure(figsize=(16,16))\n","  for i in range(8): # only display first 8 feature maps (channels)\n","    f = out[:,:,i]\n","    plt.subplot(1,8,i+1)\n","    plt.imshow(f,cmap='gray')\n","    plt.axis('off')\n","    plt.title(\"{0:.2f}\".format(f.min()) + \"/\" + \"{0:.2f}\".format(f.max()))\n","\n","# See conv_base.summary() for complete list of layer names\n","show_feature_maps_from_layer(layer_name='conv1')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"blR0tq57Ba22"},"source":["Because this layer (``conv1``) is a convolution layer, each feature map results from applying a filter to the input image. As you can see, different filters highlight different features of the input image. The next layer in the network then takes these feature maps and transforms them somehow to extract new and more abstract features. In one of the later layers (``conv_dw_2``) we can still sort-of see the cat in the feature maps:"]},{"cell_type":"code","metadata":{"id":"CFmjLmkZAsBE"},"source":["show_feature_maps_from_layer(layer_name='conv_dw_2')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ghwv2iw6C3kD"},"source":["In the last feature map it is impossible for us to see that there was originally a cat in the input image:"]},{"cell_type":"code","metadata":{"id":"zve01efLDD1p"},"source":["show_feature_maps_from_layer(layer_name='conv_pw_13_relu')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XrvSiAmIDRgy"},"source":["### Question 5.1\n","1. Notice that the height and width of the feature maps become smaller and smaller as we move deeper into the network. Why do you think that is?\n","2. If the last feature map is only three pixels high and three pixels wide (shape is ``3x3x1024``), how can the neural network know that there is a cat in the image, i.e, where is that information encoded in the feature map?"]},{"cell_type":"markdown","metadata":{"id":"EIjrfD4ecsOS"},"source":["##Task 6: How to use the image generator \n","Loading and preprocessing images is such a common task in deep learning that frameworks like Keras provide predefined tools for us that we can use. \n","In this task we will look at Keras' image data generator: https://keras.io/preprocessing/image/#imagedatagenerator-class. Simply put, the image generator is a tool that makes loading and preprocessing data easy.\n","\n","Let's set up an image generator that outputs mini-batches of 8 images:\n"]},{"cell_type":"code","metadata":{"id":"CP9tCD3Je5WP"},"source":["from keras.preprocessing.image import ImageDataGenerator\n","datagen = ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n","generator = datagen.flow_from_directory(str(p), # this is where you specify the path to the main data folder\n","                                        target_size=(120,120),\n","                                        color_mode='rgb',\n","                                        batch_size=8,\n","                                        class_mode='categorical',\n","                                        shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sya17-HqWD1z"},"source":["**Note:** Check that the classes assigned by the generator are consistent with your class assignment:"]},{"cell_type":"code","metadata":{"id":"qpd9vZt6WNrs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630823595894,"user_tz":-120,"elapsed":221,"user":{"displayName":"Henrik Pedersen","photoUrl":"","userId":"16528122701399724846"}},"outputId":"7b66076a-38d0-474a-9666-d0ef269564de"},"source":["print('generator:',generator.class_indices)\n","print('your\\'s:',dict((class_name,class_index) for class_index,class_name in enumerate(classes)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["generator: {'cat': 0, 'dog': 1, 'horse': 2}\n","your's: {'cat': 0, 'dog': 1, 'horse': 2}\n"]}]},{"cell_type":"markdown","metadata":{"id":"5g4FSIy_fg5Q"},"source":["Here is one way to generate a new batch:"]},{"cell_type":"code","metadata":{"id":"YC9X6GshfLYv"},"source":["inputs, labels = generator.next()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P535xuM7e9jL"},"source":["### Questions 6.1\n","1. What is variable ``inputs``? (Hint: look at the shape)\n","2. What is variable ``labels``?\n","3. How does the image generator know where the images are stored?\n","4. How does the image generator know the class of each image?\n","5. What does shuffle mean?"]},{"cell_type":"markdown","metadata":{"id":"6_hQXxhFgLaJ"},"source":["As you learned in lecture 2 it is always a good idea to split the data into a training set and a validation set. Again, this is such a common task in deep learning that the image generator can do it for us:"]},{"cell_type":"code","metadata":{"id":"uXA-IppWgYsi"},"source":["datagen = ImageDataGenerator(preprocessing_function=preprocess_input,validation_split=0.2)\n","\n","train_generator = datagen.flow_from_directory(str(p),\n","                                        target_size=(120,120),\n","                                        color_mode='rgb',\n","                                        batch_size=8,\n","                                        class_mode='categorical',\n","                                        shuffle=True,\n","                                        subset='training')\n","validation_generator = datagen.flow_from_directory(str(p),\n","                                        target_size=(120,120),\n","                                        color_mode='rgb',\n","                                        batch_size=8,\n","                                        class_mode='categorical',\n","                                        shuffle=True,\n","                                        subset='validation')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tLpdkt3ehCI0"},"source":["### Questions 6.2\n","1. How does each of the two generators know if it should produce training or validation images?\n","2. What is the validation percentage in this example?"]},{"cell_type":"markdown","metadata":{"id":"5yfcUFNAhSBS"},"source":["## Task 7: Feature extraction\n","Our goal is to train a machine learning model to correctly classify images belonging to the chosen categories (cat, dog, horse). Today, we will be using KNN, which operates on vectors. Therefore, we have to convert our images into vectors.\n","\n","We have two choices of features:\n","\n","1. Raw pixel values\n","2. MobileNet features\n","\n","Besides the features we will of course also need the labels (cat, dog, horse).\n","\n","Let's create the datasets that we will be using:\n"]},{"cell_type":"code","metadata":{"id":"pwc0rObZJ05v"},"source":["def extract_features(generator):\n","  generator.reset()\n","  raw_pixel_features_list = []\n","  mobilenet_features_list = []\n","  labels_list = []\n","  batch_index = 0\n","  while batch_index <= generator.batch_index:\n","    # Load mini-batch\n","    raw_pixels, labels = generator.next()\n","\n","    # Run through MobileNet encoder\n","    mobilenet_features = conv_base.predict(raw_pixels) # This is where we apply the CNN\n","\n","    # Vectorize images\n","    bs,h,w,c = raw_pixels.shape\n","    raw_pixels = np.reshape(raw_pixels,(bs,h*w*c)) # vectorize\n","    \n","    # Vectorize MobileNet features\n","    bs,h,w,c = mobilenet_features.shape\n","    mobilenet_features = np.reshape(mobilenet_features,(bs,h*w*c)) # vectorize\n","    \n","    # Convert one-hot encoding to class index\n","    labels = np.argmax(labels,axis=1)\n","\n","    # Save in lists\n","    for i in range(bs):\n","      raw_pixel_features_list.append(raw_pixels[i])\n","      mobilenet_features_list.append(mobilenet_features[i])\n","      labels_list.append(labels[i])\n","      \n","    batch_index = batch_index + 1\n","\n","  # Convert lists to numpy arrays\n","  raw_pixel_features = np.asarray(raw_pixel_features_list)\n","  mobilenet_features = np.asarray(mobilenet_features_list)\n","  labels = np.asarray(labels_list)\n","\n","  return raw_pixel_features, mobilenet_features, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5S9FZc68heuV"},"source":["train_features_raw, train_features_mobilenet, train_labels = extract_features(train_generator)\n","validation_features_raw, validation_features_mobilenet, validation_labels = extract_features(validation_generator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jEx-I6B20U0P"},"source":["Show the first 32 images of the validation data set:"]},{"cell_type":"code","metadata":{"id":"plz9liatrfhW"},"source":["def denormalize(input_img):\n","  '''\n","    input_img has intensities in range -1 to 1 (after Keras MobileNet preprocessing)\n","    output_img has intensities in range 0 to 1 (float)\n","  '''\n","  output_img = (input_img+1) / 2\n","  return output_img\n","\n","def vec2img(img_as_vec,output_shape=(120,120,3)):\n","  '''\n","    img_as_vec is a vectorized color image\n","    output_shape is the desired output image shape\n","  '''\n","  img_as_array = np.reshape(img_as_vec,output_shape)\n","  return img_as_array\n","\n","plt.figure(figsize=(9,9))\n","for i in range(32):\n","  # reshape feature vector into 120x120x3 array (image) and de-normalize intensities to range 0 to 1.\n","  img = vec2img(validation_features_raw[i,:])\n","  img = denormalize(img)\n","  plt.subplot(4,8,i+1)\n","  plt.imshow(img)\n","  plt.xticks([]), plt.yticks([])\n","  plt.title(classes[int(validation_labels[i])])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-AAEN6DRUIKu"},"source":["**Sub-task:** Verify that the labels above are correct"]},{"cell_type":"markdown","metadata":{"id":"r64k-mEESwzV"},"source":["### Questions 7.1\n","The image data has now been vectorized.\n","1. What is the shape of ``train_features_raw``?\n","2. What is the shape of ``train_features_mobilenet``?\n","3. What is the difference between ``train_features_raw`` and ``train_features_mobilenet``, i.e., what do they represent?\n","4. What is the shape of ``train_labels``?\n","5. How many training samples do we have, and how many validation samples?"]},{"cell_type":"markdown","metadata":{"id":"nsyddx2zjdlL"},"source":["##Task 8: Classify images using K-Nearest Neighbours (K-NN) classifier\n","Your task is to train a K-NN classifier on the training set, and evaluate the performace on the validation set by calculating the accuracy (and remember to calculate the accuracy on the validation set, not the training set!).\n","\n","First solve the task using the raw pixels as features:\n","\n","```\n","# Training set\n","train_features_raw, train_labels\n","\n","# Validation set\n","validation_features_raw, validation_labels\n","```\n","\n","Then solve the same task using the MobileNet features:\n","\n","```\n","# Training set\n","train_features_mobilenet, train_labels\n","\n","# Validation set\n","validation_features_mobilenet, validation_labels\n","```\n","\n","Compare the results and explain the difference.\n","\n","You are on your own here, but **you don't have to implement K-NN yourself**. I suggest you use [scikit-learn](https://scikit-learn.org) (and Google)."]},{"cell_type":"markdown","metadata":{"id":"xgy3bOwKwFrA"},"source":["##Task 9: K-means clustering\n","So what we have learned so far is that images of the same class tend to group closer together when using MobileNet's feature representation, but not so much when using the raw intensities. This confirms that using the raw pixels as features is in general not a good idea.\n","\n","The reason that MobileNet's feature representation works better is because the network has learned to map images onto a manifold. A manifold is kind of like a low-dimensional surface that exists in a high-dimensional space. For instance if images of faces were to be mapped into a 4D manifold, the first axis on the manifold could represent gender, and the others could represent age, view angle, and eye color. You can read more about manifold learning in chapter 5.11.3 of [the book](https://github.com/janishar/mit-deep-learning-book-pdf).\n","\n","The underlying hypothesis of using K-NN to classifiy images using MobileNet features is that *objects that are similar will map to the approximate same location on some manifold.* Here we will perform K-means clustering and verify that this is in fact the case. For the record, recall that the K-means method is an *undersupervised learning method*, so it doesn't know anything about the class labels.\n","\n","**Your task** is to perform K-means clustering twice on your training dataset: first using the raw intensity features (``train_features_raw``), then using the MobileNet features (``train_features_mobilenet``). Use as many clusters as you have classes, which is 3 if you also used cat, dog, and horse.\n","\n","For each cluster, print the class labels (cat, dog, horse) or the class indices (0, 1, 2) of all images in that cluster. Explain what you observe and compare between MobileNet features and raw pixel intensities.\n","\n","**Note** that we wont be needing the validation set in this task. Why? Because K-means is an unsupervised learning method often used to perform exploratory data analysis. Here we are pretending that we dont know the true labels. We are just interested in seeing if the image data form clusters or not.\n","\n","Again, **you don't have to implement K-means clustering from scratch**. You can use scikit-learn (and Google).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GwL76U482A9A"},"source":["##Task 10: Image retrieval\n","Implement an image search engine (this is also called image retrieval). Use MobileNet features, and if you have time do a comparison with raw pixel features.\n","\n","The search engine could work like this:\n","\n","1. Given an input image from the validation set, pre-process it and feed it through MobileNet to calculate the feature vector.\n","2. Then perform a K-NN search with K=10 against the feature vectors in the training set.\n","3. Then return the corresponding 10 closest images.\n","\n","In principle we should preprocess the input image like this:\n","```\n","img = image.load_img(some_image_path, target_size=(120, 120))\n","img_data = image.img_to_array(img)\n","img_data = np.expand_dims(img_data, axis=0)\n","img_preprocessed = preprocess_input(img_data)\n","```\n","But it is okay to cheat and use the image data that have already been preprocessed and run through MobileNet.\n","\n","**Question:** What kind of search results do you expect to see when using MobileNet features vs. using raw pixel features?"]},{"cell_type":"markdown","metadata":{"id":"oYeEks0j_Tky"},"source":["##Task 11: Transfer learning\n"]},{"cell_type":"markdown","metadata":{"id":"UfOBC8RNXByP"},"source":["Putting your own K-NN classifier on top of a pre-trained CNN is not really optimal. Why? Because, while the features of the convolutional base are better than using raw pixel values, they are not guaranteed to 100% optimal for your specific task. So, a better solution is to attach a second neural network on top of the convolutional base, and train both the classifier *and* the convolutional base at the same time. This is called **transfer learning**. The extra neural network that is put on top of our encoder is often referred to as a \"decoder\" or a \"classification head\".\n","\n","Recall that CNNs like AlexNet and MobileNet have been trained on ImageNet, which contains 1000 classes. If you download Keras' pre-trained models *including the top layers* (i.e., the decoder), the top layers are in fact the classifier that we want to replace. Let's verify this:"]},{"cell_type":"code","metadata":{"id":"Kj1lyzlHXND0"},"source":["mobilenet_full = MobileNet(weights='imagenet',\n","                      include_top=True,\n","                      input_shape=(224, 224, 3))\n","mobilenet_full.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MrTizMfWXmyX"},"source":["###Questions 11.1\n","Inspect the printout above.\n","1.  Can you identify the convolutional base of this network? (Compare to the ```conv_base``` model we used earlier.)\n","2. All layers beyond the convolutional base represent the classifier (or decoder). How many classes are there?\n","3. So what is the size of the output of the model?\n","4. Can you guess how we should interpret the output of model?\n","5. The input size must be 224 by 224 pixels (you can verify for yourself). Why do you think that is? "]},{"cell_type":"markdown","metadata":{"id":"iRtmTVrn2U8v"},"source":["So, how do we modify and re-train MobileNet to work on our own data? First of all, we don't want to train CNNs from scratch, since this could take days. Secondly, we need to modify the network architecture to output three class labels (cat, dog, horse) instead of 1000.\n","\n","The main hypothesis underlying transfer learning is that the network weights learned in the convolutional layers (i.e., the *encoder*) are generic and need little or no fine-tuning to work on other data sets or tasks. So in practice, we just need to replace and re-train the last layers (i.e., the *decoder*) of a pre-trained network.\n","\n","So let's take our convolutional base (encoder) and put a simple neural network classifier (decoder) on top of it. Your task is to figure out what the value of variable N should be."]},{"cell_type":"code","metadata":{"id":"uAtJ3XgK5tTG"},"source":["N = ???"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K1OMmP__03MB"},"source":["from keras.layers import Dense,GlobalAveragePooling2D\n","from keras.models import Model\n","\n","# Add new top layer\n","x = conv_base.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(1024,activation='relu')(x) #dense layer\n","preds = Dense(N,activation='softmax')(x) #final layer with softmax activation\n","\n","# Specify model\n","model = Model(inputs=conv_base.input, outputs=preds)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j7jBi1s-KbE2"},"source":["Note that the weights of the new dense layers are initialized with random values. So we need to train the dense layers on our dataset to make it work.\n","\n","###Questions 11.2\n","1. What should N be in the above code block?\n","2. Re-run the code block with the correct N.\n","3. What does GlobalAveragePooling2D do?\n","\n","Hint: You can print all layers and print properties like name, type and input shape:"]},{"cell_type":"code","metadata":{"id":"B36lhXo7E5-6"},"source":["for i,layer in enumerate(model.layers):\n","  layer_name = layer.name\n","  layer_type = layer.__class__.__name__\n","  input_shape = layer.input_shape\n","  print(f\"Layer {i} has name {layer_name} and type {layer_type}, and its input shape is {input_shape}\")\n","  \n","# Or use the summary function:\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZO_YKGvp20I2"},"source":["We will only be training the new dense layers that we added. Disable training for all previous layers and enable for new layers:"]},{"cell_type":"code","metadata":{"id":"FL_pekjgUGnz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630833181519,"user_tz":-120,"elapsed":231,"user":{"displayName":"Henrik Pedersen","photoUrl":"","userId":"16528122701399724846"}},"outputId":"d247f299-7b67-4132-a95a-6aa2e2324771"},"source":["total_num_layers = len(model.layers)\n","num_base_layers = len(conv_base.layers)\n","print(f\"Total number of layers is {total_num_layers}\")\n","print(f\"Number of pretrained base layers is {num_base_layers}\")\n","\n","for layer in model.layers[:num_base_layers]:\n","    layer.trainable=False\n","for layer in model.layers[num_base_layers:]:\n","    layer.trainable=True"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of layers is 89\n","Number of pretrained base layers is 86\n"]}]},{"cell_type":"markdown","metadata":{"id":"1acWCuVD3xvI"},"source":["We are now ready to start training the model using\n","- Adam optimizer\n","- loss function will be categorical cross entropy\n","- evaluation metric will be accuracy\n"]},{"cell_type":"code","metadata":{"id":"_qXOMhrGM36f"},"source":["from tensorflow.keras import optimizers\n","\n","# Set up optimizer\n","sgd_optimizer = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n","\n","# Compile model - make it trainable\n","model.compile(optimizer=sgd_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","step_size_train = train_generator.n//train_generator.batch_size # Number of mini-batches per epoch (training)\n","step_size_val = validation_generator.n//validation_generator.batch_size # Number of mini-batches per epoch (validation)\n","\n","# Train model for 10 epochs\n","history = model.fit_generator(generator=train_generator,\n","                   validation_data=validation_generator,\n","                   validation_steps=step_size_val,\n","                   steps_per_epoch=step_size_train,\n","                   epochs=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mdAelWXnO_tG"},"source":["###Questions 11.3\n","Look at the outputs of the training.\n","\n","1. What is the difference between 'loss' and 'val_loss'?\n","2. What is the difference between 'accuracy' and 'val_accuracy'?\n","3. Do they behave the same, or do they behave differently? Try to explain what you see."]},{"cell_type":"markdown","metadata":{"id":"MBQU1wAPD4-w"},"source":["## Task 12: Deploying the model\n","Here is how to deploy the model and integrate with OpenCV."]},{"cell_type":"code","metadata":{"id":"LO1p9_2LWUMH"},"source":["import cv2\n","\n","# Pick first image of first class\n","filelist = [x for x in (p/classes[0]).iterdir() if x.is_file()]\n","img_path = str(filelist[1])\n","print(f\"File path: {img_path}\")\n","\n","# Remember to convert to RGB\n","img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n","plt.imshow(img);\n","plt.axis('off');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TZMwJJ1w_YVW"},"source":["Now, make sure that the image shape and the pixel intensity range is as expected by the network (required shape = `1x120x120x3` and intensity range from -1 to 1):"]},{"cell_type":"code","metadata":{"id":"THYIJ5n2fX_u"},"source":["img = cv2.resize(img, (120, 120))\n","img = (img[...,::-1].astype(np.float32))\n","img /= 127.5\n","img -= 1.\n","img = np.expand_dims(img,0)\n","print(img.shape,img.min(),img.max())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"52pNra-fRXRk"},"source":["Run the image through the network:"]},{"cell_type":"code","metadata":{"id":"Jeg17ghAg5H4"},"source":["class_probabilities = model.predict(img)[0]\n","print(class_probabilities)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HsU3tcsOJaz1"},"source":["# pick class with highest probability\n","class_index = (-class_probabilities).argsort()[0]\n","print(f'The image displays a {classes[class_index]}!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TtSiBK3DURiC"},"source":["##Ideas for further work\n","1. In the above example we have not optimized the pre-trained weights of the convolutional base (i.e., the encoder). To improve performance further you could enable training in all layers (including the convolutoinal base) and re-train the network. This is called *fine-tuning*.\n","2. Another way to improve model performance is by *data augmentation*. Have a look [here](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) to see what kind of augmentation is possible. Why do you think data augmentation helps improve the performance of your model?"]},{"cell_type":"markdown","metadata":{"id":"9CLHhqpL_n8M"},"source":["##Exporting to TensorFlow JS and hosting a web service on GitHub\n","If you wanted to, you could in principle deploy your model and make a nice web service like this one:\n","https://klaverhenrik.github.io/transferlearning/. The webpage is hosted on GitHub using [GitHub Pages](https://pages.github.com/)."]}]}