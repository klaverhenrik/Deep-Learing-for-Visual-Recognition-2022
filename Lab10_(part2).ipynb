{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REaV1GblZovs"
   },
   "source": [
    "# Visualization of filters by reconstruction\n",
    "In this part of the lab we consider reconstruction-based filter visualization.\n",
    "\n",
    "The content is inspired by [this blog post](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html), which unfortunately is deprecated.\n",
    "\n",
    "We will use this one instead:\n",
    "\n",
    "- Blog post: https://keras.io/examples/vision/visualizing_what_convnets_learn/\n",
    "\n",
    "- Source: https://github.com/keras-team/keras-io/blob/master/examples/vision/visualizing_what_convnets_learn.py\n",
    "\n",
    "- Colab version: https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/visualizing_what_convnets_learn.ipynb\n",
    "\n",
    "\n",
    "**Before we start - remember to set runtime to GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohj31dy5QyeA"
   },
   "source": [
    "## Set up network\n",
    "We will be using VGG16 like in Lab 10, part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Xdbdit5Q1C_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# The dimensions of our input image\n",
    "img_width = 224\n",
    "img_height = 224\n",
    "\n",
    "# Our target layer: we will visualize the filters from this layer.\n",
    "# See `model.summary()` for list of layer names, if you want to change this.\n",
    "layer_name = \"block3_conv3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zINSWTEdRO1W"
   },
   "outputs": [],
   "source": [
    "# Build a VGG16 model loaded with pre-trained ImageNet weights\n",
    "model = keras.applications.VGG16(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "# Set up a model that returns the activation values for our target layer\n",
    "layer = model.get_layer(name=layer_name)\n",
    "feature_extractor = keras.Model(inputs=model.inputs, outputs=layer.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWvdIoHFRmzw"
   },
   "source": [
    "## Set up the gradient ascent process\n",
    "The \"loss\" we will maximize is simply the mean of the activation of a specific filter in our target layer. To avoid border effects, we exclude border pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYWEpNQTRtim"
   },
   "outputs": [],
   "source": [
    "def compute_loss(input_image, filter_index):\n",
    "    activation = feature_extractor(input_image)\n",
    "    # We avoid border artifacts by only involving non-border pixels in the loss.\n",
    "    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n",
    "    return tf.reduce_mean(filter_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxeP9bCNRz9Q"
   },
   "source": [
    "Our gradient ascent function simply computes the gradients of the loss above with respect to the input image, and update the image so as to move it towards a state that will activate the target filter more strongly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9muuwZpDR9xV"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gradient_ascent_step(img, filter_index, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(img)\n",
    "        loss = compute_loss(img, filter_index)\n",
    "    # Compute gradients.\n",
    "    grads = tape.gradient(loss, img)\n",
    "    # Normalize gradients.\n",
    "    grads = tf.math.l2_normalize(grads)\n",
    "    img += learning_rate * grads\n",
    "    return loss, img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqB55jY8Wr4y"
   },
   "source": [
    "Some background reading in case you are interested: \n",
    "- https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "- https://www.tensorflow.org/guide/autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDD-J8deAV3p"
   },
   "source": [
    "## Set up the end-to-end filter visualization loop\n",
    "Our process is as follow:\n",
    "\n",
    "- Start from a random image that is close to \"all gray\" (i.e. visually netural).\n",
    "- Repeatedly apply the gradient ascent step function defined above\n",
    "- Convert the resulting input image back to a displayable form, by normalizing it, center-cropping it, and restricting it to the [0, 255] range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRInJgueScsr"
   },
   "outputs": [],
   "source": [
    "def initialize_image():\n",
    "    # We start from a gray image with some random noise\n",
    "    img = tf.random.uniform((1, img_width, img_height, 3))\n",
    "    # VGG16 expects inputs in the range [-128, +128].\n",
    "    # Here we scale our random inputs to [-32, +32]\n",
    "    return (img - 0.5) * 64\n",
    "\n",
    "\n",
    "def visualize_filter(filter_index):\n",
    "    # We run gradient ascent for 20 steps\n",
    "    iterations = 30\n",
    "    learning_rate = 200.0 # learning must be high for this wo converge fast enough\n",
    "    img = initialize_image()\n",
    "    for iteration in range(iterations):\n",
    "        loss, img = gradient_ascent_step(img, filter_index, learning_rate)\n",
    "\n",
    "    # Decode the resulting input image\n",
    "    img = deprocess_image(img[0].numpy())\n",
    "    return loss, img\n",
    "\n",
    "\n",
    "def deprocess_image(img):\n",
    "    # Normalize array: center on 0., ensure variance is 0.15\n",
    "    img -= img.mean()\n",
    "    img /= img.std() + 1e-5\n",
    "    img *= 0.15\n",
    "\n",
    "    # Center crop\n",
    "    img = img[25:-25, 25:-25, :]\n",
    "\n",
    "    # Clip to [0, 1]\n",
    "    img += 0.5\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    # Convert to RGB array\n",
    "    img *= 255\n",
    "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPVp13gnSk5q"
   },
   "source": [
    "Let's try it out with filter 0 in the target layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "TZLaOrrlSnb7",
    "outputId": "2dd73124-a4d5-4604-e9d5-490515c8f348"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "loss, img = visualize_filter(0)\n",
    "keras.preprocessing.image.save_img(\"0.png\", img)\n",
    "\n",
    "display(Image(\"0.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiNc_MgMSvah"
   },
   "source": [
    "## Visualize the first 16 filters in the target layer\n",
    "Now, let's make a 4x4 grid of the first 16 filters in the target layer to get of feel for the range of different visual patterns that the model has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "edyOQr07UVkX",
    "outputId": "c027ca40-2c1d-4ef2-f6b3-8d5432a4ccad"
   },
   "outputs": [],
   "source": [
    "# Compute image inputs that maximize per-filter activations\n",
    "# for the first 16 filters of our target layer\n",
    "all_imgs = []\n",
    "for filter_index in range(16):\n",
    "    print(\"Processing filter %d\" % (filter_index,))\n",
    "    loss, img = visualize_filter(filter_index)\n",
    "    all_imgs.append(img)\n",
    "\n",
    "# Build a black picture with enough space for\n",
    "# our 4 x 4 filters of size 128 x 128, with a 5px margin in between\n",
    "margin = 5\n",
    "n = 4\n",
    "cropped_width = img_width - 25 * 2\n",
    "cropped_height = img_height - 25 * 2\n",
    "width = n * cropped_width + (n - 1) * margin\n",
    "height = n * cropped_height + (n - 1) * margin\n",
    "stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "# Fill the picture with our saved filters\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        img = all_imgs[i * n + j]\n",
    "        stitched_filters[\n",
    "            (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,\n",
    "            (cropped_height + margin) * j : (cropped_height + margin) * j\n",
    "            + cropped_height,\n",
    "            :,\n",
    "        ] = img\n",
    "keras.preprocessing.image.save_img(\"stiched_filters.png\", stitched_filters)\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(\"stiched_filters.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uukAQ0oFZ2F"
   },
   "source": [
    "## Questions\n",
    "1. The computational graph is changed in order to derive the filter response image. How is the graph modified?\n",
    "2. How is the loss calculated? What is being maximized/minimized?\n",
    "3. Try visualizing other filters from another layer. **Note:** You may need to adjust `iterations` and `learning_rate`\n",
    "\n",
    "Use the code block below to display the names of the convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WUvLGjTCU9xf",
    "outputId": "69c69f8b-de57-4264-a50d-9c8a227b5cad"
   },
   "outputs": [],
   "source": [
    "# Print names of all conv layers\n",
    "for i, layer in enumerate(model.layers):\n",
    "  \n",
    "  # check for convolutional layer\n",
    "  layer_type = layer.__class__.__name__\n",
    "  \n",
    "  if 'Conv' not in layer_type:\n",
    "    continue\n",
    "  \n",
    "  # get filter weights\n",
    "  layer_name = layer.name\n",
    "  input_shape = layer.input_shape\n",
    "  output_shape = layer.output.shape\n",
    "  filter_shape = layer.get_weights()[0].shape\n",
    "  \n",
    "  print(f\"Layer {i} has name {layer_name}, input shape {input_shape}, filter shape {filter_shape}, and output shape {output_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01-ookNsIH3U"
   },
   "source": [
    "# Grad-CAM\n",
    "If time permits, you may also want to try out Grad-CAM.\n",
    "\n",
    "The code is based on [this tutorial](https://keras.io/examples/vision/grad_cam/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_QYgZqSiIPEu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Display\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDv3l7Y5aUkY"
   },
   "source": [
    "You can change these to another model.\n",
    "\n",
    "To get the values for last_conv_layer_name use model.summary() to see the names of all layers in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 839
    },
    "id": "m2XUcdL_ZY_H",
    "outputId": "eb06aa74-bb1d-40e2-d12f-1a8f908e3f60"
   },
   "outputs": [],
   "source": [
    "model_builder = keras.applications.vgg16.VGG16\n",
    "img_size = (224, 224)\n",
    "preprocess_input = keras.applications.vgg16.preprocess_input\n",
    "decode_predictions = keras.applications.vgg16.decode_predictions\n",
    "\n",
    "last_conv_layer_name = \"block5_conv3\"\n",
    "\n",
    "# The local path to our target image\n",
    "img_path = keras.utils.get_file(\n",
    "    \"cat.jpg\", \"https://raw.githubusercontent.com/klaverhenrik/Deep-Learning-for-Visual-Recognition-2021/main/data/cat.jpg\"\n",
    ")\n",
    "\n",
    "display(Image(img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avVS3rGSaydq"
   },
   "source": [
    "## The Grad-CAM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iiAkvdq0auMW"
   },
   "outputs": [],
   "source": [
    "def get_img_array(img_path, size):\n",
    "    # `img` is a PIL image of size 224x224\n",
    "    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n",
    "    # `array` is a float32 Numpy array of shape (224, 224, 3)\n",
    "    array = keras.preprocessing.image.img_to_array(img)\n",
    "    # We add a dimension to transform our array into a \"batch\"\n",
    "    # of size (1, 224, 224, 3)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LocIpviGbSJA"
   },
   "source": [
    "Let's test-drive it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "6Rm6QXtwbPtU",
    "outputId": "df6656e9-fe16-4243-bd15-765602e5081e"
   },
   "outputs": [],
   "source": [
    "# Prepare image\n",
    "img_array = preprocess_input(get_img_array(img_path, size=img_size))\n",
    "\n",
    "# Make model\n",
    "model = model_builder(weights=\"imagenet\")\n",
    "\n",
    "# Remove last layer's softmax\n",
    "model.layers[-1].activation = None\n",
    "\n",
    "# Print what the top predicted class is\n",
    "preds = model.predict(img_array)\n",
    "print(\"Predicted:\", decode_predictions(preds, top=1)[0])\n",
    "\n",
    "# Generate class activation heatmap\n",
    "heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
    "\n",
    "# Display heatmap\n",
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLiS4rEgbblo"
   },
   "source": [
    "Create a superimposed visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lJIVZ9QpbVpS",
    "outputId": "e01db30c-1616-4789-b69d-5d4870fcb7c0"
   },
   "outputs": [],
   "source": [
    "def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
    "    # Load the original image\n",
    "    img = keras.preprocessing.image.load_img(img_path)\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "\n",
    "    # Save the superimposed image\n",
    "    superimposed_img.save(cam_path)\n",
    "\n",
    "    # Display Grad CAM\n",
    "    display(Image(cam_path))\n",
    "\n",
    "\n",
    "save_and_display_gradcam(img_path, heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQdG5jfublFS"
   },
   "source": [
    "## Let's try another image\n",
    "We will see how the grad cam explains the model's outputs for a multi-label image. Let's try an image with a cat and a dog together, and see how the grad cam behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "7-e05emcbfIB",
    "outputId": "8c9dbdc1-b7ed-490e-bca4-736b63aec5f7"
   },
   "outputs": [],
   "source": [
    "img_path = keras.utils.get_file(\n",
    "    \"cat_and_dog.jpg\",\n",
    "    \"https://storage.googleapis.com/petbacker/images/blog/2017/dog-and-cat-cover.jpg\",\n",
    ")\n",
    "\n",
    "display(Image(img_path))\n",
    "\n",
    "# Prepare image\n",
    "img_array = preprocess_input(get_img_array(img_path, size=img_size))\n",
    "\n",
    "# Print what the two top predicted classes are\n",
    "preds = model.predict(img_array)\n",
    "print(\"Predicted:\", decode_predictions(preds, top=2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "U-YVS-2mbpbk",
    "outputId": "7c0130bf-1c4e-4b99-b6a5-9a7989d30048"
   },
   "outputs": [],
   "source": [
    "# dog heatmap\n",
    "heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=260)\n",
    "save_and_display_gradcam(img_path, heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667
    },
    "id": "6jmn-6xSbwhj",
    "outputId": "2ff25937-c2dc-4fce-fa13-3f9afd8a32e0"
   },
   "outputs": [],
   "source": [
    "# Cat heatmap\n",
    "heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=285)\n",
    "save_and_display_gradcam(img_path, heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJBlCmh8b5w9"
   },
   "source": [
    "## Question\n",
    "1. The computational graph is changed in order to compute the heatmaps. How is the graph modified?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional task - test your knowledge!\n",
    "We are approaching the end of the course, and if you have followed the lessons and solved the labs, you should be able to read and understand deep learning research papers and code.\n",
    "\n",
    "To test your knowledge, pick one or more of the official Keras code examples and see of you can figure out how they work. All of the code examples have a link to a Google Colab notebook that you can run.\n",
    "\n",
    "**All code examples:** https://keras.io/examples/\n",
    "\n",
    "**Computer vision examples:** https://keras.io/examples/vision/\n",
    "\n",
    "**Examples of generative models:** https://keras.io/examples/generative/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzO7Ct83b009"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab 10 (part 2) Solution",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
